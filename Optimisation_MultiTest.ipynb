{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import sklearn\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from csv import writer\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.callback import Callback\n",
    "from pymoo.factory import get_selection\n",
    "from pymoo.core.selection import Selection\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation, get_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify list of start and end date for historical periods to use in paramater estimation for desired number of optimisations\n",
    "\n",
    "history_start_list = ['2017-01-01','2018-01-01','2019-01-01','2020-01-01','2021-01-01']\n",
    "history_end_list = ['2018-01-01','2019-01-01','2020-01-01','2021-01-01','2022-01-01']\n",
    "\n",
    "return_difference_list = []\n",
    "result_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, value in enumerate(history_start_list):\n",
    "\n",
    "\n",
    "\n",
    "    history_start_date = value\n",
    "    history_end_date = history_end_list[count]\n",
    "\n",
    "    # Specify algorithm and constraint parameters\n",
    "\n",
    "    cardinality_constraint = 5\n",
    "    cardinality_threshold = 0\n",
    "    max_asset_weight = 0.3\n",
    "    population_size = 100\n",
    "    number_of_generations = 10000\n",
    "    \n",
    "    portfolio_holding_period = 15 # Time to hold selected portfolio in calendar days\n",
    "\n",
    "\n",
    "\n",
    "    conn = sqlite3.connect('continuous_futures.db')\n",
    "\n",
    "    excluded_assets = ('CME_SP1_FW', 'CME_DA1_FW', 'CME_RS11_FW')\n",
    "\n",
    "    assets = pd.read_sql(f\"SELECT date, quandl_code, settle FROM \\\n",
    "    Continuous_Futures WHERE method = 'FW' AND depth = 1 AND exchange = 'CME' AND quandl_code NOT IN {excluded_assets} AND \\\n",
    "    date BETWEEN '{history_start_date}' AND '{history_end_date}'\", conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    assets.set_index('date', inplace = True)\n",
    "    assets_p = assets.pivot(columns=\"quandl_code\", values=\"settle\")\n",
    "    assets_p.sort_values(by = 'date', ascending = False, inplace = True)\n",
    "\n",
    "    # Check for any extreme proportion of missing prices within chosen range for each asset\n",
    "    # print(assets_p.isna().sum() / len(assets_p)) \n",
    "\n",
    "    asset_list1 = assets_p.columns[0:].to_list()\n",
    "    daily_returns = pd.DataFrame()\n",
    "\n",
    "    assets_p.dropna(inplace = True)\n",
    "\n",
    "    for i in asset_list1:\n",
    "        daily_returns[f'{i}_long'] = ((assets_p[f'{i}'] - assets_p[f'{i}'].shift(-1)) / assets_p[f'{i}'].shift(-1))\n",
    "        daily_returns[f'{i}_short'] = ((assets_p[f'{i}'].shift(-1) - assets_p[f'{i}']) / assets_p[f'{i}'].shift(-1))\n",
    "\n",
    "    daily_returns.dropna(inplace = True)\n",
    "\n",
    "    # Create vector of mean daily return for each asset, long & short\n",
    "\n",
    "    mean_return = []\n",
    "\n",
    "    for i in asset_list1:\n",
    "        long_return = daily_returns[f'{i}_long'].mean()\n",
    "        short_return = daily_returns[f'{i}_short'].mean()\n",
    "        mean_return.append(long_return)\n",
    "        mean_return.append(short_return)\n",
    "\n",
    "    mreturn = np.array(mean_return)\n",
    "\n",
    "    output_csv_name = f\"history{value}to{history_end_date}_pop{population_size}_ngen{number_of_generations}_K{cardinality_constraint}_Max{max_asset_weight}_{len(mreturn) / 2}assets_{count}\"\n",
    "\n",
    "    # Sample covariance matrix\n",
    "    # covariance_matrix = daily_returns.cov().to_numpy()\n",
    "\n",
    "    # Apply Ledoit Wolf shrinkage to improve covariance matrix estimation\n",
    "\n",
    "    LW_shrinkage = LedoitWolf().fit(daily_returns)\n",
    "    cvm = LW_shrinkage.covariance_\n",
    "\n",
    "    # Generate initial population for cardinality & maximum weighting constrained test\n",
    "\n",
    "    maxweight_cardinality_list = []\n",
    "\n",
    "    while len(maxweight_cardinality_list) < population_size:\n",
    "        x = np.random.dirichlet(np.ones(cardinality_constraint), size = 1)\n",
    "        if np.all(x <= max_asset_weight):\n",
    "            maxweight_cardinality_list.append(x)\n",
    "\n",
    "    initial_pop = np.zeros((population_size, len(mreturn)))\n",
    "\n",
    "    for i in range(population_size):\n",
    "        initial_pop[i,:cardinality_constraint] = maxweight_cardinality_list[i]\n",
    "        np.random.shuffle(initial_pop[i])\n",
    "\n",
    "    # Sample population to satisfy cardinality constraint alone\n",
    "\n",
    "    # initial_pop = np.zeros((population_size, len(mreturn)))\n",
    "\n",
    "    # for i in range(population_size):\n",
    "    #     initial_pop[i,:cardinality_constraint] = np.random.rand(cardinality_constraint)\n",
    "    #     initial_pop[i] = initial_pop[i] / np.sum(initial_pop[i])\n",
    "    #     np.random.shuffle(initial_pop[i])\n",
    "\n",
    "\n",
    "    class MyProblem(ElementwiseProblem):\n",
    "\n",
    "        def __init__(self, mreturn, cvm):\n",
    "            super().__init__(n_var = len(mreturn),\n",
    "                            n_obj = 2,\n",
    "                            n_constr = 2,\n",
    "                            xl = np.array([0 for asset in mreturn]),\n",
    "                            xu = np.array([1 for asset in mreturn]))\n",
    "\n",
    "            self.mreturn = mreturn\n",
    "            self.cvm = cvm\n",
    "            self.K = cardinality_constraint\n",
    "\n",
    "        def _evaluate(self, x, out, *args, **kwargs):\n",
    "            \n",
    "            # Normalise asset weightings to collectively sum to 1\n",
    "            x = x / np.sum(x) \n",
    "\n",
    "            # Risk/variance objective function\n",
    "            f1 = np.dot(x, (np.dot(self.cvm, x))) \n",
    "            \n",
    "            # Mean return objective function\n",
    "            f2 = -(np.dot(x, self.mreturn))\n",
    "\n",
    "            # Cardinality Constraint\n",
    "            g1 = sum(asset > cardinality_threshold for asset in x) - self.K\n",
    "\n",
    "            # Require asset weightings sum to 1 +/- threshold.  Unnecessary with normalisation\n",
    "            # g2 = ((sum(x) - 1)) ** 2 - asset_sum_threshold \n",
    "            \n",
    "            # Maximum asset weighting constraint\n",
    "            g3 = (max((asset - max_asset_weight) for asset in x))\n",
    "\n",
    "            # Minimum asset weighting constraint\n",
    "            # g4 = (max((min_asset_weight - asset) for asset in x if asset > cardinality_threshold))\n",
    "\n",
    "            # Pre-assignment constraint\n",
    "            #g5 = 0.25 - x[16]\n",
    "            \n",
    "            out[\"F\"] = np.column_stack([f1, f2])\n",
    "            out[\"G\"] = [g1, g3]\n",
    "\n",
    "\n",
    "    problem = MyProblem(mreturn, cvm)\n",
    "\n",
    "\n",
    "    algorithm = NSGA2(pop_size = population_size, sampling = initial_pop)\n",
    "\n",
    "\n",
    "    res = minimize(problem,\n",
    "                algorithm,\n",
    "                (\"n_gen\", number_of_generations),\n",
    "                verbose = True,\n",
    "                save_history = False,\n",
    "                seed = None)\n",
    "\n",
    "    result_list.append(res.F)\n",
    "\n",
    "    # Create CSV to import for headers of optimised portfolio output\n",
    "\n",
    "    asset_headings = list(daily_returns.columns)\n",
    "\n",
    "    asset_headings.append(\"Variance\")\n",
    "    asset_headings.append(\"Mean Return\")\n",
    "\n",
    "    with open(f\"{output_csv_name}.csv\", 'w', newline = '') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow(asset_headings)\n",
    "        f_object.close()\n",
    "\n",
    "    result_X_normalised = [x / np.sum(x) for x in res.X] # normalise output portfolio compositions that make up non-dominated solutions to obtain correct weightings\n",
    "\n",
    "    # Convert mean return objective values which have been minimised to positive\n",
    "\n",
    "    for i in res.F:\n",
    "        i[1] *= (-1)\n",
    "\n",
    "    result_x_list = []\n",
    "\n",
    "    for count, value in enumerate(result_X_normalised):\n",
    "        for i in value:\n",
    "            result_x_list.append(i)\n",
    "        for j in res.F[count]:\n",
    "            result_x_list.append(j)\n",
    "\n",
    "    chunked_result_x_list = []\n",
    "    chunk_size = len(asset_headings)\n",
    "\n",
    "    for i in range(0, len(result_x_list), chunk_size):\n",
    "        chunked_result_x_list.append(result_x_list[i:i + chunk_size])\n",
    "\n",
    "    with open(f'{output_csv_name}.csv', 'a', newline='') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        for i in chunked_result_x_list:\n",
    "            writer_object.writerow(i)\n",
    "        f_object.close()\n",
    "\n",
    "    print(history_end_date)\n",
    "\n",
    "# Calculate difference between realised returns over holding period relative to optimised values\n",
    "\n",
    "    holding_period_start = (datetime.strptime(history_end_date, '%Y-%m-%d') + timedelta(days = 1)).strftime('%Y-%m-%d')\n",
    "    holding_period_end = (datetime.strptime(history_end_date, '%Y-%m-%d') + timedelta(days = portfolio_holding_period)).strftime('%Y-%m-%d')\n",
    "\n",
    "    conn = sqlite3.connect('continuous_futures.db')\n",
    "\n",
    "    portfolio_results = pd.read_sql(f\"SELECT date, quandl_code, settle FROM \\\n",
    "    Continuous_Futures WHERE method = 'FW' AND depth = 1 AND exchange = 'CME' \\\n",
    "    AND date BETWEEN '{holding_period_start}' AND '{holding_period_end}'\", conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    portfolio_results.set_index('date', inplace = True)\n",
    "    portfolio_results = portfolio_results.pivot(columns=\"quandl_code\", values = \"settle\")\n",
    "    portfolio_results.sort_values(by = 'date', ascending = False, inplace = True)\n",
    "\n",
    "    portfolio_assets_list = portfolio_results.columns[0:].to_list()\n",
    "    portfolio_returns = pd.DataFrame()\n",
    "\n",
    "    for i in portfolio_assets_list:\n",
    "        portfolio_returns[f'{i}_long'] = ((portfolio_results[f'{i}'] - portfolio_results[f'{i}'].shift(-1)) / portfolio_results[f'{i}'].shift(-1))\n",
    "        portfolio_returns[f'{i}_short'] = ((portfolio_results[f'{i}'].shift(-1) - portfolio_results[f'{i}']) / portfolio_results[f'{i}'].shift(-1))\n",
    "\n",
    "    portfolio_returns.dropna(inplace = True)\n",
    "\n",
    "    pf_mean_return = []\n",
    "\n",
    "    for i in asset_list1:\n",
    "        long_return = portfolio_returns[f'{i}_long'].mean()\n",
    "        short_return = portfolio_returns[f'{i}_short'].mean()\n",
    "        pf_mean_return.append(long_return)\n",
    "        pf_mean_return.append(short_return)\n",
    "\n",
    "    pf_er_list = []\n",
    "\n",
    "    for i in result_X_normalised:\n",
    "        pf_er_list.append(sum(i * pf_mean_return))\n",
    "        (sum(i * pf_mean_return))\n",
    "\n",
    "\n",
    "    total_results = pd.read_csv(f\"{output_csv_name}.csv\")\n",
    "\n",
    "    total_results[f'{portfolio_holding_period} Day Return'] = pf_er_list\n",
    "    total_results['Difference vs. Mean Return'] = total_results[f'{portfolio_holding_period} Day Return'] - total_results['Mean Return']\n",
    "    return_difference_list.append(total_results['Difference vs. Mean Return'].mean())\n",
    "\n",
    "    total_results.to_csv(f'{output_csv_name}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the set of non-dominated solution objective values from each test\n",
    "\n",
    "for count, value in enumerate(result_list):\n",
    "    np.savetxt(f\"ResultsF_{count}.csv\", value, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of Non-Dominated Solution set from Each Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_list = ['goldenrod', 'firebrick', 'mediumseagreen', 'plum', 'navy']\n",
    "label_list = ['2017', '2018', '2019', '2020', '2021']\n",
    "\n",
    "plt.rc('font', size = 10)\n",
    "plt.rc('axes', titlesize = 10, labelsize = 9)\n",
    "plt.rc('axes', facecolor = 'white')\n",
    "plt.rc('xtick', labelsize = 7)\n",
    "plt.rc('ytick', labelsize = 7)\n",
    "plt.figure(figsize=(6.5, 4), facecolor = 'white')\n",
    "\n",
    "for count, value in enumerate(result_list):\n",
    "        plt.scatter((value[:, 0]), (value[:, 1]), s = 5, marker = \"o\", facecolors = colour_list[count], label = label_list[count])\n",
    "\n",
    "plt.xlabel(\"Variance of Returns\")\n",
    "plt.ylabel(\"Mean Daily Return\")\n",
    "plt.legend(fancybox = True, shadow = True, fontsize = 9, markerscale = 3, loc = 4)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Optimisation_Paretofront.png\", dpi = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
